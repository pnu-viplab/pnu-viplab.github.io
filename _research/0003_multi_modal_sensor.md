---
title: Multi-Modal Sensor Fusion based 3D Depth Perception
layout: post
slug: 0003-multi-modal-sensor
images:
  - /assets/img/research/0003_multi_modal_sensor_01.png
  - /assets/img/research/0003_multi_modal_sensor_02.gif
description: "다중 모달 센서 융합 기반 3D 깊이 인식 연구"
---

<img src="/assets/img/research/0003_multi_modal_sensor_01.png" style="width:100%;" />
<img src="/assets/img/research/0003_multi_modal_sensor_02.gif" style="width:100%;" />

## Abstract

The objective of depth estimation is to generate dense depth predictions based on various input information, such as a single RGB image, multi-view images, sparse LiDAR measurements, and so on. Depth perception has become an important problem in recent years with the rapid growth of computer vision applications, such as augmented reality, unmanned aerial vehicle control, autonomous driving, and motion planning. To obtain a reliable depth prediction, information from various sensors is utilized, _e.g._, RGB cameras, radar, LiDAR, and ultrasonic sensors.
